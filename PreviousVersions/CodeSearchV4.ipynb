{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66b2295",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7b3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.9/site-packages (2.14.6)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./.local/lib/python3.9/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.local/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.9/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea02f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c06b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in ./.local/lib/python3.9/site-packages (1.7.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.9/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (0.17.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (4.35.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from sentence-transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (1.8)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (15.0.7)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.9/site-packages (from networkx->torch>=1.6.0->sentence-transformers) (5.0.9)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29006d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c73f85",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5471bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataset['train']['func_documentation_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8c327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e0c5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0da703",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b98a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(model.get_sentence_embedding_dimension()))\n",
    "index.add_with_ids(encoded_data, np.array(range(0, len(documents))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serializing index to export it across different host\n",
    "faiss.write_index(index, 'sample_documents')\n",
    "\n",
    "#de-serializing the index\n",
    "index = faiss.read_index('sample_documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36615516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version that is ordered by document id\n",
    "\n",
    "def semantic_search(query):\n",
    "    t = time.time()\n",
    "    query_vector = model.encode([query])\n",
    "    \n",
    "    # Search for all results (remove the 'k' limit)\n",
    "    top_k = index.search(query_vector, index.ntotal)\n",
    "    \n",
    "    # Extract document IDs and scores for all documents\n",
    "    document_ids = top_k[1].tolist()[0]\n",
    "    scores = top_k[0].tolist()[0]\n",
    "    \n",
    "    # Create a list of tuples containing document IDs and scores for all documents\n",
    "    results = [(doc_id, score) for doc_id, score in zip(document_ids, scores)]\n",
    "    \n",
    "    # Sort the results by document ID\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Return just the scores\n",
    "    semantic_scores = [i[1] for i in results]\n",
    "\n",
    "    # normalize semantic scores\n",
    "    max_score = max(semantic_scores)\n",
    "    normalized_semantic_scores = [score / max_score for score in semantic_scores]\n",
    "    normalized_semantic_scores\n",
    "    \n",
    "    return normalized_semantic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search(\"enumerable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d0897",
   "metadata": {},
   "source": [
    "## BM-25 Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_tokens = dataset['train']['func_code_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(func_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(user_input):\n",
    "    \n",
    "    doc_scores = bm25.get_scores(user_input)\n",
    "    max_score = max(doc_scores)\n",
    "\n",
    "    # normalize BM25 scores\n",
    "    normalized_doc_scores = [score / max_score for score in doc_scores]\n",
    "    \n",
    "    return normalized_doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fadd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_search(\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc699e7",
   "metadata": {},
   "source": [
    "## Combined Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_results(user_input):\n",
    "    \n",
    "#     s = semantic_search(user_input)\n",
    "#     t = tfidf_search(user_input)\n",
    "#     overlap_results = list(set(s) & set(t))\n",
    "    \n",
    "#     top_10_docs = overlap_results\n",
    "        \n",
    "#     while len(top_10_docs) < 10:\n",
    "#         for i in s:\n",
    "#             if i not in top_10_docs:\n",
    "#                 top_10_docs.append(i)\n",
    "    \n",
    "#     if len(top_10_docs) > 10:\n",
    "#         top_10_docs = overlap_results[:10]\n",
    "        \n",
    "#     function_name = []\n",
    "#     doc_string = []\n",
    "#     for i in top_10_docs:\n",
    "#         function_name.append(dataset['train']['func_name'][i])\n",
    "#         doc_string.append(dataset['train']['func_documentation_string'][i])\n",
    "        \n",
    "#     results_df = pd.DataFrame({'Document': top_10_docs, 'Function': function_name, 'Documentation': doc_string})\n",
    "    \n",
    "#     return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187de7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quartiles(data):\n",
    "    # Filter out zeros and sort the remaining data\n",
    "    filtered_sorted_data = sorted([x for x in data if x != 0])\n",
    "\n",
    "    n = len(filtered_sorted_data)\n",
    "    if n == 0:\n",
    "        # Handle the case where all values are zero\n",
    "        return [0 for _ in data]\n",
    "\n",
    "    # Calculate quartile breakpoints\n",
    "    q1 = filtered_sorted_data[int(n * 0.25) - 1]\n",
    "    q2 = filtered_sorted_data[int(n * 0.5) - 1]\n",
    "    q3 = filtered_sorted_data[int(n * 0.75) - 1]\n",
    "\n",
    "    # Assign quartiles including zeros\n",
    "    quartiles = []\n",
    "    for value in data:\n",
    "        if value <= q1:\n",
    "            quartiles.append(0)\n",
    "        elif value <= q2:\n",
    "            quartiles.append(1)\n",
    "        elif value <= q3:\n",
    "            quartiles.append(2)\n",
    "        else:\n",
    "            quartiles.append(3)\n",
    "\n",
    "    return quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06170e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_results(sem_weight, bm_weight, user_input):\n",
    "    \n",
    "    sem = semantic_search(user_input)\n",
    "    bm = bm25_search(user_input)\n",
    "    weighted_sem = [i * sem_weight for i in sem]\n",
    "    weighted_bm = [i * bm_weight for i in bm]\n",
    "    weighted_avg = [weighted_sem[i]+ weighted_bm[i] for i in range(0, len(weighted_bm))]\n",
    "    sum_weight = sem_weight + bm_weight \n",
    "    weighted_avg_norm = [i/sum_weight for i in weighted_avg]\n",
    "    url = dataset[\"train\"]['func_code_url']\n",
    "    if not weighted_avg_norm or np.isnan(weighted_avg_norm).any():\n",
    "        # Handle the empty or invalid input case\n",
    "        return {} \n",
    "    \n",
    "    try:\n",
    "#         import math\n",
    "#         labels = pd.qcut(weighted_avg_norm, q=4, labels=False, duplicates='drop')\n",
    "#         output_dict = {url[i]: labels[i] for i in range(len(weighted_avg_norm))}\n",
    "        labels = find_quartiles(weighted_avg_norm)\n",
    "        output_dict = {url[i]: labels[i] for i in range(len(weighted_avg_norm))}\n",
    "        \n",
    "    except ValueError:\n",
    "        # Handle the case where qcut fails\n",
    "        return {}\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53724e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(search_output, eval_dict):\n",
    "    inter = list(search_output.keys() & eval_dict.keys())\n",
    "    y_pred = [search_output[i] for i in search_output.keys() if i in inter]\n",
    "    y_true = [eval_dict[i] for i in eval_dict.keys() if i in inter]\n",
    "    \n",
    "    if (len(y_true) <=1):\n",
    "        return None\n",
    "    \n",
    "    if (len(y_pred) <=1):\n",
    "        return None\n",
    "    \n",
    "    from sklearn.metrics import ndcg_score\n",
    "        \n",
    "    y_true_nd = np.zeros(shape=(len(y_true), 4))\n",
    "    y_true_nd[np.arange(len( y_true)), y_true] = 1\n",
    "    # Transform predictions to ndarray\n",
    "    y_pred_nd = np.zeros(shape=(len(y_true), 4))\n",
    "    y_pred_nd[np.arange(len( y_true)), y_pred] = 1\n",
    "    return ndcg_score(y_true_nd, y_pred_nd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"annotationStore.csv\") \n",
    "eval_Python = eval_df[eval_df[\"Language\"] == \"Python\"]\n",
    "# sub = eval_ruby[eval_ruby['Query'] == 'enummerable']\n",
    "# area_dict = dict(zip(sub.GitHubUrl, sub.Relevance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndcg(search_results(0.5 , 0.5, 'enummerable'), area_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in eval_ruby['Query']:\n",
    "#     trys(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision(search_output, eval_dict):\n",
    "    \n",
    "#     inter = list(search_output.keys() & eval_dict.keys())\n",
    "#     predicted_scores = [search_output[i] for i in search_output.keys() if i in inter]\n",
    "#     true_scores = [eval_dict[i] for i in eval_dict.keys() if i in inter]\n",
    "    \n",
    "    \n",
    "#     def is_relevant(score):\n",
    "#         return score >= 2  \n",
    "\n",
    "#     sorted_items = sorted(zip(predicted_scores, true_scores), reverse=True, key=lambda x: x[0])\n",
    "\n",
    "#     top_k_items = sorted_items\n",
    "\n",
    "#     relevant_count = sum(1 for _, true_score in top_k_items if is_relevant(true_score))\n",
    "#     return relevant_count/len(top_k_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e283dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision(search_output, eval_dict):\n",
    "    \n",
    "#     inter = list(search_output.keys() & eval_dict.keys())\n",
    "#     predicted_scores = [search_output[i] for i in search_output.keys() if i in inter]\n",
    "#     true_scores = [eval_dict[i] for i in eval_dict.keys() if i in inter]\n",
    "    \n",
    "    \n",
    "#     def TP_def(pscore, tscore):\n",
    "#         if pscore >= 2:\n",
    "#             if tscore >= 2:\n",
    "#                 return True\n",
    "#         else:\n",
    "#             return False\n",
    "        \n",
    "#     def TN_def(pscore, tscore):\n",
    "#         if pscore < 2:\n",
    "#             if tscore < 2:\n",
    "#                 return True\n",
    "#         else:\n",
    "#             return False\n",
    "    \n",
    "    \n",
    "#     sorted_items = sorted(zip(predicted_scores, true_scores), reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "#     TP = sum(1 for tp, ts in sorted_items if TP_def(tp,ts))\n",
    "#     TN = sum(1 for tp, ts in sorted_items if TN_def(tp,ts))\n",
    "\n",
    "#     return (TP+TN)/len(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b13645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_weights(sem_weight, bm_weight):\n",
    "#     prec = []\n",
    "#     for i in list(eval_ruby[\"Query\"].unique()):\n",
    "#         subset = eval_ruby[eval_ruby[\"Query\"] == i]\n",
    "#         if subset.shape[0] == 0:\n",
    "#             continue\n",
    "#         evals = pd.Series(subset.Relevance.values,index=subset.GitHubUrl).to_dict()\n",
    "#         our_search = search_results(sem_weight, bm_weight, i)\n",
    "#         inter = list(evals.keys() & our_search.keys())\n",
    "#         if len(inter)==0:\n",
    "#             continue\n",
    "#         prec.append(precision(our_search, evals))\n",
    "#         print(len(prec))\n",
    "#     return sum(prec)/len(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd907bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_weights(sem_weight, bm_weight):\n",
    "\n",
    "    grouped_evals = eval_Python.groupby('Query').apply(lambda x: pd.Series(x.Relevance.values, index=x.GitHubUrl).to_dict())\n",
    "\n",
    "    ndcgs = []\n",
    "    search_results_cache = {}  \n",
    "\n",
    "    for query, evals in grouped_evals.items():\n",
    "        if (sem_weight, bm_weight, query) not in search_results_cache:\n",
    "            search_results_cache[(sem_weight, bm_weight, query)] = search_results(sem_weight, bm_weight, query)\n",
    "        \n",
    "        our_search = search_results_cache[(sem_weight, bm_weight, query)]\n",
    "#         inter = set(evals.keys()) & set(our_search.keys())\n",
    "\n",
    "#         if inter:\n",
    "#             a = precision(our_search, evals)\n",
    "#             prec.append(a)\n",
    "        ndcgs.append(ndcg(our_search, grouped_evals[query]))\n",
    "    filtered_scores = [s for s in ndcgs if s is not None]\n",
    "    return  sum(filtered_scores)/len(filtered_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def grid_search(sem_weight_range, increment):\n",
    "    best_precision = 0\n",
    "    best_weights = (0, 0)\n",
    "    \n",
    "\n",
    "    for sem_weight in np.arange(*sem_weight_range, increment):\n",
    "        bm_weight = 5 - sem_weight\n",
    "        current_precision = find_best_weights(sem_weight, bm_weight)\n",
    "        if current_precision > best_precision:\n",
    "            best_precision = current_precision\n",
    "            best_weights = (sem_weight, bm_weight)\n",
    "        print(sem_weight, bm_weight)\n",
    "        print(current_precision)\n",
    "\n",
    "    return best_weights, best_precision\n",
    "\n",
    "# Example usage\n",
    "sem_weight_range = (0, 5)  # Define the range for sem_weight \n",
    "increment = 1           # Define the increment\n",
    "\n",
    "best_weights, best_precision = grid_search(sem_weight_range, increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b9979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c97de9",
   "metadata": {},
   "source": [
    "# End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "ae48c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end():\n",
    "    grouped_evals = eval_Python.groupby('Query').apply(lambda x: pd.Series(x.Relevance.values, index=x.GitHubUrl).to_dict())\n",
    "\n",
    "    prec = []\n",
    "    search_results_cache = {}  \n",
    "\n",
    "    sem_weight, bm_weight = best_weights\n",
    "    \n",
    "    for query, evals in grouped_evals.items():\n",
    "        if (sem_weight, bm_weight, query) not in search_results_cache:\n",
    "            search_results_cache[(sem_weight, bm_weight, query)] = search_results(sem_weight, bm_weight, query)\n",
    "        \n",
    "        our_search = search_results_cache[(sem_weight, bm_weight, query)]\n",
    "        inter = set(evals.keys()) & set(our_search.keys())\n",
    "\n",
    "        if inter:\n",
    "            prec.append(precision(our_search, evals))\n",
    "    return sum(prec) / len(prec) if prec else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "40f74399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6674242424242425"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_to_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbcc08",
   "metadata": {},
   "source": [
    "## Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efc7eb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Function</th>\n",
       "      <th>Documentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34424</td>\n",
       "      <td>Pandata.DataFormatter.custom_sort</td>\n",
       "      <td>Sorts alphabetically ignoring the initial 'The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>Twitter.Utils.flat_pmap</td>\n",
       "      <td>Returns a new array with the concatenated resu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3891</td>\n",
       "      <td>Twitter.Utils.pmap</td>\n",
       "      <td>Returns a new array with the results of runnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38981</td>\n",
       "      <td>TeradataExtractor.Query.enumerable</td>\n",
       "      <td>returns an enumerable, each element of which i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43259</td>\n",
       "      <td>Doublylinkedlist.Doublylinkedlist.each</td>\n",
       "      <td>Método para que la lista sea enumerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42390</td>\n",
       "      <td>Yargi.ElementSet.grep</td>\n",
       "      <td>See Enumerable.grep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42720</td>\n",
       "      <td>MMETools.Enumerable.classify</td>\n",
       "      <td>Interessant iterador que classifica un enumera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30079</td>\n",
       "      <td>Wbem.WsmanClient.each_instance</td>\n",
       "      <td>Enumerate instances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46812</td>\n",
       "      <td>StixSchemaSpy.SimpleType.enumeration_values</td>\n",
       "      <td>Returns the list of values for this enumeration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5229</td>\n",
       "      <td>Magick.ImageList.reject</td>\n",
       "      <td>override Enumerable's reject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document                                     Function  \\\n",
       "0     34424            Pandata.DataFormatter.custom_sort   \n",
       "1      3890                      Twitter.Utils.flat_pmap   \n",
       "2      3891                           Twitter.Utils.pmap   \n",
       "3     38981           TeradataExtractor.Query.enumerable   \n",
       "4     43259       Doublylinkedlist.Doublylinkedlist.each   \n",
       "5     42390                        Yargi.ElementSet.grep   \n",
       "6     42720                 MMETools.Enumerable.classify   \n",
       "7     30079               Wbem.WsmanClient.each_instance   \n",
       "8     46812  StixSchemaSpy.SimpleType.enumeration_values   \n",
       "9      5229                      Magick.ImageList.reject   \n",
       "\n",
       "                                       Documentation  \n",
       "0  Sorts alphabetically ignoring the initial 'The...  \n",
       "1  Returns a new array with the concatenated resu...  \n",
       "2  Returns a new array with the results of runnin...  \n",
       "3  returns an enumerable, each element of which i...  \n",
       "4            Método para que la lista sea enumerable  \n",
       "5                                See Enumerable.grep  \n",
       "6  Interessant iterador que classifica un enumera...  \n",
       "7                                Enumerate instances  \n",
       "8    Returns the list of values for this enumeration  \n",
       "9                       override Enumerable's reject  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results(\"enumerable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
